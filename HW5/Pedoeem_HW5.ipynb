{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_hB0ONqo7fBP"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "huFAATqX7fB3"
   },
   "outputs": [],
   "source": [
    "LONGEST_ARTICLE = 190\n",
    "NUM_CLASSES = 4\n",
    "lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2prqGGPv7fCT"
   },
   "outputs": [],
   "source": [
    "#preprocess the data \n",
    "def pre_data(filename):\n",
    "    train_sentences = []\n",
    "    train_y = []\n",
    "    with open(filename, \"r\") as data: \n",
    "        line = data.readline()\n",
    "        while line:\n",
    "            y,title,body = line.split(\"\\\",\\\"\")\n",
    "            y = int(y[1:])-1\n",
    "            text =\"{} {}\".format(title,body).split(\" \")\n",
    "            text = [stemmer.stem(t.strip()) for t in text] \n",
    "            train_sentences.append(text)\n",
    "            train_y.append(y)\n",
    "            line = data.readline()\n",
    "    return train_sentences, train_y\n",
    "                \n",
    "#know longest article is 197 words and that there are 141,206 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yQN9eTdT7fCn"
   },
   "outputs": [],
   "source": [
    "train_sentences, y = pre_data(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NDflcpkm7fDc"
   },
   "outputs": [],
   "source": [
    "train_sentencesNP = np.array(train_sentences)\n",
    "yC = keras.utils.to_categorical(y)\n",
    "randos = np.random.choice(len(train_sentencesNP),len(train_sentencesNP),replace=False)\n",
    "validation_randos = randos[:20000]\n",
    "train_randos = randos[20000:]\n",
    "train_x = train_sentencesNP[train_randos]\n",
    "train_y = yC[train_randos]\n",
    "val_x = train_sentencesNP[validation_randos]\n",
    "val_y = yC[validation_randos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mREuw4JO7fD2"
   },
   "outputs": [],
   "source": [
    "train_x = np.append(train_x, \"ENDOFSENTENCETOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZQb7yVVT7fEO"
   },
   "outputs": [],
   "source": [
    "t = keras.preprocessing.text.Tokenizer()\n",
    "t.fit_on_texts(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a70GPh7t7fEm"
   },
   "outputs": [],
   "source": [
    "train_sequences = t.texts_to_sequences(train_x)\n",
    "validation_sequences = t.texts_to_sequences(val_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xjKKZuy47fFG"
   },
   "outputs": [],
   "source": [
    "EOS_TOKEN = train_sequences.pop()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UJ2BvL9y7fGH"
   },
   "outputs": [],
   "source": [
    "def add_padding(sequences, EOS_TOKEN, MAX_LENGTH):\n",
    "    padded_sequences = np.zeros((len(sequences),MAX_LENGTH))\n",
    "    for seq in tqdm(range(len(sequences))):\n",
    "        len_seq = len(sequences[seq])\n",
    "        if len_seq>MAX_LENGTH:\n",
    "            padded_sequences[seq] =  sequences[seq][:MAX_LENGTH]\n",
    "        else:\n",
    "            padded_sequences[seq] =  sequences[seq] + (MAX_LENGTH - len_seq )*[EOS_TOKEN]\n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cOIAtuPp7fGX",
    "outputId": "53e3f5a8-e58b-4c3e-9760-0256185bd992"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:01<00:00, 53531.04it/s]\n"
     ]
    }
   ],
   "source": [
    "padded_sequences = add_padding(train_sequences,EOS_TOKEN,LONGEST_ARTICLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gRJSWh7_7fGx",
    "outputId": "7346fab6-df86-4d4a-d038-4a7ba8eff98b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:00<00:00, 51193.41it/s]\n"
     ]
    }
   ],
   "source": [
    "padded_val_sequences = add_padding(validation_sequences, EOS_TOKEN,LONGEST_ARTICLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4s69axCh7fHF"
   },
   "outputs": [],
   "source": [
    "##THIS IS A LIST OF OTHER MODELS THAT DID JUST A TAD BETTER THAN THE SMALL LIST, BUT WITH MANY MORE PARAMETERS\n",
    "#with out any convolutions get some good results, after 6 epochs get 91.6% on val set.  \n",
    "# model = keras.Sequential()\n",
    "# model.add(keras.layers.Embedding(len(t.word_index)+1,512, input_length=LONGEST_ARTICLE))\n",
    "# model.add(keras.layers.Flatten())\n",
    "# model.add(keras.layers.Dense(NUM_CLASSES,activation=\"softmax\"))\n",
    "# model.compile(optimizer=keras.optimizers.Adam(lr),\n",
    "#               loss='categorical_crossentropy',\n",
    "#               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gy0GYNGh9VMQ"
   },
   "outputs": [],
   "source": [
    "#This is also pretty good get, after 6 epochs get 91%\n",
    "# model = keras.Sequential()\n",
    "# model.add(keras.layers.Embedding(len(t.word_index)+1,256, input_length=LONGEST_ARTICLE))\n",
    "# model.add(keras.layers.Conv1D(filters=32,kernel_size=4, dilation_rate=8, padding='valid',activation='elu'))\n",
    "# model.add(keras.layers.Flatten())\n",
    "# model.add(keras.layers.Dense(NUM_CLASSES,activation=\"softmax\"))\n",
    "# model.compile(optimizer=keras.optimizers.Adam(lr),\n",
    "#               loss='categorical_crossentropy',\n",
    "#               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i61D8AYo93eU"
   },
   "outputs": [],
   "source": [
    "#this is an attempt to make a really small model, after 32 epochs get 90.8% on val.\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(len(t.word_index)+1,4, input_length=LONGEST_ARTICLE))\n",
    "model.add(keras.layers.Conv1D(filters=32,kernel_size=2, dilation_rate=2, padding='valid',activation='relu'))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(NUM_CLASSES,activation=\"softmax\"))\n",
    "model.compile(optimizer=keras.optimizers.Adam(lr),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1458
    },
    "colab_type": "code",
    "id": "JvkE8SNT7fHX",
    "outputId": "eee6065a-40d4-4f59-cd91-f24757c9373a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 190, 4)            510492    \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 188, 32)           288       \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 6016)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 4)                 24068     \n",
      "=================================================================\n",
      "Total params: 534,848\n",
      "Trainable params: 534,848\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100000 samples, validate on 20000 samples\n",
      "Epoch 1/32\n",
      "100000/100000 [==============================] - 3s 30us/step - loss: 1.3823 - acc: 0.3040 - val_loss: 1.3698 - val_acc: 0.3694\n",
      "Epoch 2/32\n",
      "100000/100000 [==============================] - 2s 23us/step - loss: 1.2981 - acc: 0.5543 - val_loss: 1.1759 - val_acc: 0.6733\n",
      "Epoch 3/32\n",
      "100000/100000 [==============================] - 2s 24us/step - loss: 1.0067 - acc: 0.7320 - val_loss: 0.8458 - val_acc: 0.7753\n",
      "Epoch 4/32\n",
      "100000/100000 [==============================] - 2s 23us/step - loss: 0.7247 - acc: 0.8032 - val_loss: 0.6360 - val_acc: 0.8208\n",
      "Epoch 5/32\n",
      "100000/100000 [==============================] - 2s 25us/step - loss: 0.5610 - acc: 0.8422 - val_loss: 0.5186 - val_acc: 0.8460\n",
      "Epoch 6/32\n",
      "100000/100000 [==============================] - 2s 24us/step - loss: 0.4620 - acc: 0.8644 - val_loss: 0.4445 - val_acc: 0.8639\n",
      "Epoch 7/32\n",
      "100000/100000 [==============================] - 2s 23us/step - loss: 0.3969 - acc: 0.8799 - val_loss: 0.3979 - val_acc: 0.8750\n",
      "Epoch 8/32\n",
      "100000/100000 [==============================] - 2s 23us/step - loss: 0.3523 - acc: 0.8917 - val_loss: 0.3649 - val_acc: 0.8838\n",
      "Epoch 9/32\n",
      "100000/100000 [==============================] - 2s 23us/step - loss: 0.3193 - acc: 0.9006 - val_loss: 0.3423 - val_acc: 0.8879\n",
      "Epoch 10/32\n",
      "100000/100000 [==============================] - 2s 23us/step - loss: 0.2937 - acc: 0.9076 - val_loss: 0.3249 - val_acc: 0.8932\n",
      "Epoch 11/32\n",
      "100000/100000 [==============================] - 2s 23us/step - loss: 0.2731 - acc: 0.9139 - val_loss: 0.3114 - val_acc: 0.8964\n",
      "Epoch 12/32\n",
      "100000/100000 [==============================] - 2s 23us/step - loss: 0.2556 - acc: 0.9189 - val_loss: 0.3007 - val_acc: 0.8997\n",
      "Epoch 13/32\n",
      "100000/100000 [==============================] - 2s 23us/step - loss: 0.2408 - acc: 0.9237 - val_loss: 0.2925 - val_acc: 0.9016\n",
      "Epoch 14/32\n",
      "100000/100000 [==============================] - 2s 23us/step - loss: 0.2278 - acc: 0.9277 - val_loss: 0.2852 - val_acc: 0.9037\n",
      "Epoch 15/32\n",
      "100000/100000 [==============================] - 2s 23us/step - loss: 0.2160 - acc: 0.9313 - val_loss: 0.2792 - val_acc: 0.9066\n",
      "Epoch 16/32\n",
      "100000/100000 [==============================] - 2s 23us/step - loss: 0.2055 - acc: 0.9347 - val_loss: 0.2749 - val_acc: 0.9077\n",
      "Epoch 17/32\n",
      "100000/100000 [==============================] - 2s 23us/step - loss: 0.1959 - acc: 0.9378 - val_loss: 0.2707 - val_acc: 0.9086\n",
      "Epoch 18/32\n",
      "100000/100000 [==============================] - 2s 23us/step - loss: 0.1869 - acc: 0.9409 - val_loss: 0.2673 - val_acc: 0.9105\n",
      "Epoch 19/32\n",
      "100000/100000 [==============================] - 2s 23us/step - loss: 0.1787 - acc: 0.9430 - val_loss: 0.2648 - val_acc: 0.9110\n",
      "Epoch 20/32\n",
      "100000/100000 [==============================] - 2s 23us/step - loss: 0.1710 - acc: 0.9460 - val_loss: 0.2621 - val_acc: 0.9122\n",
      "Epoch 21/32\n",
      "100000/100000 [==============================] - 2s 23us/step - loss: 0.1635 - acc: 0.9483 - val_loss: 0.2604 - val_acc: 0.9113\n",
      "Epoch 22/32\n",
      "100000/100000 [==============================] - 2s 23us/step - loss: 0.1566 - acc: 0.9508 - val_loss: 0.2592 - val_acc: 0.9136\n",
      "Epoch 23/32\n",
      "100000/100000 [==============================] - 2s 23us/step - loss: 0.1499 - acc: 0.9530 - val_loss: 0.2575 - val_acc: 0.9139\n",
      "Epoch 24/32\n",
      "100000/100000 [==============================] - 2s 23us/step - loss: 0.1436 - acc: 0.9552 - val_loss: 0.2569 - val_acc: 0.9149\n",
      "Epoch 25/32\n",
      "100000/100000 [==============================] - 2s 23us/step - loss: 0.1377 - acc: 0.9568 - val_loss: 0.2561 - val_acc: 0.9132\n",
      "Epoch 26/32\n",
      "100000/100000 [==============================] - 2s 23us/step - loss: 0.1320 - acc: 0.9586 - val_loss: 0.2553 - val_acc: 0.9146\n",
      "Epoch 27/32\n",
      "100000/100000 [==============================] - 2s 23us/step - loss: 0.1263 - acc: 0.9605 - val_loss: 0.2556 - val_acc: 0.9131\n",
      "Epoch 28/32\n",
      "100000/100000 [==============================] - 2s 23us/step - loss: 0.1211 - acc: 0.9626 - val_loss: 0.2555 - val_acc: 0.9142\n",
      "Epoch 29/32\n",
      "100000/100000 [==============================] - 2s 23us/step - loss: 0.1161 - acc: 0.9643 - val_loss: 0.2557 - val_acc: 0.9145\n",
      "Epoch 30/32\n",
      "100000/100000 [==============================] - 2s 23us/step - loss: 0.1111 - acc: 0.9663 - val_loss: 0.2567 - val_acc: 0.9143\n",
      "Epoch 31/32\n",
      "100000/100000 [==============================] - 2s 23us/step - loss: 0.1064 - acc: 0.9676 - val_loss: 0.2573 - val_acc: 0.9139\n",
      "Epoch 32/32\n",
      "100000/100000 [==============================] - 2s 23us/step - loss: 0.1019 - acc: 0.9690 - val_loss: 0.2583 - val_acc: 0.9152\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1543d126a0>"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary()\n",
    "model.fit(padded_sequences,train_y, epochs = 32, batch_size = 512, validation_data=(padded_val_sequences,val_y),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BhVOugW5JFQY"
   },
   "outputs": [],
   "source": [
    "def pre_data_test(filename):\n",
    "    test_sentences = []\n",
    "    test_y = []\n",
    "    with open(filename, \"r\") as data: \n",
    "        line = data.readline()\n",
    "        while line:\n",
    "            y,title,body = line.split(\"\\\",\\\"\")\n",
    "            y = int(y[1:])-1\n",
    "            text =\"{} {}\".format(title,body).split(\" \")\n",
    "            text = [stemmer.stem(t.strip()) for t in text] \n",
    "            test_sentences.append(text)\n",
    "            test_y.append(y)\n",
    "            line = data.readline()\n",
    "    return test_sentences, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WiJqKvxv7fNA",
    "outputId": "3a4cf695-b492-42fe-ce05-ef098277cdcd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7600/7600 [00:00<00:00, 52505.23it/s]\n"
     ]
    }
   ],
   "source": [
    "test_x,test_y = pre_data_test(\"test.csv\")\n",
    "test_sequences = t.texts_to_sequences(test_x)\n",
    "test_yC = keras.utils.to_categorical(test_y)\n",
    "padded_test = add_padding(test_sequences,EOS_TOKEN,LONGEST_ARTICLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "JDvfYrj_IRlc",
    "outputId": "9408b780-8bca-4b79-d0d0-353c510d64fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7600/7600 [==============================] - 1s 92us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.25492076643987704, 0.9153947368421053]"
      ]
     },
     "execution_count": 55,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(padded_test, test_yC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DVkAhd2LJONy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "agpA0lZxJOux"
   },
   "source": [
    "Test Accuracy of Small Model :  91.5%\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Pedoeem_HW5.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
