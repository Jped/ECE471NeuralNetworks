{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "LONGEST_ARTICLE = 190\n",
    "NUM_CLASSES = 4\n",
    "lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess the data \n",
    "def pre_data(filename):\n",
    "    train_sentences = []\n",
    "    train_y = []\n",
    "    with open(filename, \"r\") as data: \n",
    "        line = data.readline()\n",
    "        while line:\n",
    "            y,title,body = line.split(\"\\\",\\\"\")\n",
    "            y = int(y[1:])-1\n",
    "            text =\"{} {}\".format(title,body).split(\" \")\n",
    "            text = [stemmer.stem(t.strip()) for t in text] \n",
    "            train_sentences.append(text)\n",
    "            train_y.append(y)\n",
    "            line = data.readline()\n",
    "    return train_sentences, train_y\n",
    "                \n",
    "#know longest article is 197 words and that there are 141,206 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_data_test(filename):\n",
    "    test_sentences = []\n",
    "    test_y = []\n",
    "    with open(filename, \"r\") as data: \n",
    "        line = data.readline()\n",
    "        while line:\n",
    "            y,title,body = line.split(\"\\\",\\\"\")\n",
    "            y = int(y[1:])-1\n",
    "            text =\"{} {}\".format(title,body).split(\" \")\n",
    "            text = [stemmer.stem(t.strip()) for t in text] \n",
    "            test_sentences.append(text)\n",
    "            test_y.append(y)\n",
    "            line = data.readline()\n",
    "    return test_sentences, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences, y = pre_data(\"/home/jonny/Documents/CurroML/HW5/ag-news-csv/ag_news_csv/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentencesNP = np.array(train_sentences)\n",
    "yC = keras.utils.to_categorical(y)\n",
    "randos = np.random.choice(len(train_sentencesNP),len(train_sentencesNP),replace=False)\n",
    "validation_randos = randos[:20000]\n",
    "train_randos = randos[20000:]\n",
    "train_x = train_sentencesNP[train_randos]\n",
    "train_y = yC[train_randos]\n",
    "val_x = train_sentencesNP[validation_randos]\n",
    "val_y = yC[validation_randos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.append(train_x, \"ENDOFSENTENCETOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = keras.preprocessing.text.Tokenizer()\n",
    "t.fit_on_texts(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = t.texts_to_sequences(train_x)\n",
    "validation_sequences = t.texts_to_sequences(val_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = train_sequences.pop()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_padding(sequences, EOS_TOKEN, MAX_LENGTH):\n",
    "    padded_sequences = np.zeros((len(sequences),MAX_LENGTH))\n",
    "    for seq in tqdm(range(len(sequences))):\n",
    "        len_seq = len(sequences[seq])\n",
    "        if len_seq>MAX_LENGTH:\n",
    "            padded_sequences[seq] =  sequences[seq][:MAX_LENGTH]\n",
    "        else:\n",
    "            padded_sequences[seq] =  sequences[seq] + (MAX_LENGTH - len_seq )*[EOS_TOKEN]\n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:01<00:00, 52629.87it/s]\n"
     ]
    }
   ],
   "source": [
    "padded_sequences = add_padding(train_sequences,EOS_TOKEN,LONGEST_ARTICLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:00<00:00, 60200.11it/s]\n"
     ]
    }
   ],
   "source": [
    "padded_val_sequences = add_padding(validation_sequences, EOS_TOKEN,LONGEST_ARTICLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##THT IS A LIST OF OTHER MODELS THAT DID JUST A TAD BETTER THAN th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is an attempt to make a really small model, after 32 epochs get 90.8% on val.\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(len(t.word_index)+1,4, input_length=LONGEST_ARTICLE))\n",
    "model.add(keras.layers.Conv1D(filters=32,kernel_size=2, dilation_rate=2, padding='valid',activation='elu'))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(NUM_CLASSES,activation=\"softmax\"))\n",
    "model.compile(optimizer=keras.optimizers.Adam(lr),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_32 (Embedding)     (None, 190, 4)            510900    \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 188, 32)           288       \n",
      "_________________________________________________________________\n",
      "flatten_28 (Flatten)         (None, 6016)              0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 4)                 24068     \n",
      "=================================================================\n",
      "Total params: 535,256\n",
      "Trainable params: 535,256\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 100000 samples, validate on 20000 samples\n",
      "Epoch 1/16\n",
      "100000/100000 [==============================] - 16s 162us/step - loss: 0.5371 - acc: 0.7967 - val_loss: 0.5580 - val_acc: 0.7873\n",
      "Epoch 2/16\n",
      "100000/100000 [==============================] - 18s 175us/step - loss: 0.5186 - acc: 0.8051 - val_loss: 0.5416 - val_acc: 0.7950\n",
      "Epoch 3/16\n",
      "100000/100000 [==============================] - 17s 166us/step - loss: 0.5016 - acc: 0.8123 - val_loss: 0.5289 - val_acc: 0.8010\n",
      "Epoch 4/16\n",
      "100000/100000 [==============================] - 17s 168us/step - loss: 0.4863 - acc: 0.8190 - val_loss: 0.5151 - val_acc: 0.8070\n",
      "Epoch 5/16\n",
      "100000/100000 [==============================] - 18s 176us/step - loss: 0.4723 - acc: 0.8255 - val_loss: 0.5037 - val_acc: 0.8120\n",
      "Epoch 6/16\n",
      " 37888/100000 [==========>...................] - ETA: 9s - loss: 0.4601 - acc: 0.8306"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "model.fit(padded_sequences,train_y, epochs = 16, batch_size = 512, validation_data=(padded_val_sequences,val_y),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7600/7600 [00:00<00:00, 55178.66it/s]\n"
     ]
    }
   ],
   "source": [
    "test_x,test_y = pre_data_test(\"/home/jonny/Documents/CurroML/HW5/ag-news-csv/ag_news_csv/test.csv\")\n",
    "test_sequences = t.texts_to_sequences(test_x)\n",
    "test_yC = keras.utils.to_categorical(test_y)\n",
    "padded_test = add_padding(test_sequences,EOS_TOKEN,LONGEST_ARTICLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7600/7600 [==============================] - 1s 82us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5904176346879256, 0.7734210526315789]"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This final accuracy is on the SMALL MODEL \n",
    "model.evaluate(padded_test, test_yC)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
